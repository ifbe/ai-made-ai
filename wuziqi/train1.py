import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))  # å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pickle
import random
import numpy as np
from collections import defaultdict
from model import FearGreedWuziqiModel
from game import (
    check_win, get_legal_moves, get_nearby_moves,
    BOARD_SIZE, BLACK, WHITE, EMPTY, BOARD_POSITIONS
)

def get_best_device():
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡ï¼šMLX (Apple Silicon) > CUDA > CPU"""
    try:
        if torch.backends.mps.is_available():
            print("âœ… ä½¿ç”¨ Apple Silicon (MPS)")
            return torch.device("mps")
    except:
        pass
    
    if torch.cuda.is_available():
        print("âœ… ä½¿ç”¨ NVIDIA GPU (CUDA)")
        return torch.device("cuda")
    
    print("âš ï¸ ä½¿ç”¨ CPU")
    return torch.device("cpu")

class Stage1Trainer:
    def __init__(self, model, device, lr=5e-5):
        self.model = model
        self.device = device
        self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=50)
        
        if device.type == 'mps':
            print("   MPS æ¨¡å¼: ä½¿ç”¨æ›´ç¨³å®šçš„è®¾ç½®")
            torch.backends.mps.enable_fallback_to_cpu = True
    
    def train_step(self, batch):
        boards = []
        turns = []
        actions = []
        rewards = []
        legal_moves_list = []
        
        for item in batch:
            boards.append(item['board'])
            turns.append(item['turn'])
            actions.append(item['move'])
            rewards.append(item['reward'])
            
            nearby = get_nearby_moves(item['board'], distance=2)
            if not nearby:
                center = BOARD_SIZE // 2
                nearby = [center * BOARD_SIZE + center]
            legal_moves_list.append(nearby)
        
        boards = torch.tensor(boards, dtype=torch.long, device=self.device)
        turns = torch.tensor(turns, device=self.device)
        actions = torch.tensor(actions, device=self.device)
        rewards = torch.tensor(rewards, dtype=torch.float, device=self.device)
        
        policy_logits, values = self.model.forward_with_mask(
            boards, turns, legal_moves=legal_moves_list
        )
        
        policy_logits = torch.clamp(policy_logits, -20, 20)
        
        log_probs = F.log_softmax(policy_logits, dim=-1)
        action_log_probs = log_probs.gather(1, actions.unsqueeze(1)).squeeze()
        
        advantages = rewards - values.squeeze().detach()
        advantages = torch.clamp(advantages, -1, 1)
        
        policy_loss = -(action_log_probs * advantages).mean()
        value_loss = F.mse_loss(values.squeeze(), rewards)
        
        entropy = 0
        for b in range(len(batch)):
            valid_indices = legal_moves_list[b]
            if valid_indices:
                valid_logits = policy_logits[b][valid_indices]
                valid_probs = F.softmax(valid_logits, dim=-1)
                valid_log_probs = F.log_softmax(valid_logits, dim=-1)
                entropy += -(valid_probs * valid_log_probs).sum()
        entropy = entropy / len(batch)
        entropy_bonus = 0.0005 * entropy
        
        total_loss = policy_loss + 0.5 * value_loss - entropy_bonus
        
        if torch.isnan(total_loss):
            return {
                'total_loss': 0.0,
                'policy_loss': 0.0,
                'value_loss': 0.0,
                'entropy': 0.0,
                'avg_reward': rewards.mean().item()
            }
        
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)
        self.optimizer.step()
        
        return {
            'total_loss': total_loss.item(),
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'entropy': entropy.item(),
            'avg_reward': rewards.mean().item()
        }
    
    def train_epoch(self, dataset, batch_size=32):
        self.model.train()
        epoch_stats = defaultdict(float)
        batch_count = 0
        valid_batches = 0
        
        indices = list(range(len(dataset)))
        random.shuffle(indices)
        
        for start_idx in range(0, len(dataset), batch_size):
            batch_indices = indices[start_idx:start_idx + batch_size]
            batch = [dataset[i] for i in batch_indices]
            stats = self.train_step(batch)
            
            if stats['total_loss'] != 0.0:
                for k, v in stats.items():
                    epoch_stats[k] += v
                valid_batches += 1
            batch_count += 1
        
        if valid_batches > 0:
            for k in epoch_stats:
                epoch_stats[k] /= valid_batches
        
        return epoch_stats
    
    def evaluate(self, num_games=5):
        """è¯„ä¼°æ¨¡å‹å¯¹æˆ˜èƒ½åŠ›"""
        self.model.eval()
        
        wins = {BLACK: 0, WHITE: 0}
        draws = 0
        
        for _ in range(num_games):
            board = [EMPTY] * BOARD_POSITIONS
            current = BLACK
            
            center = BOARD_SIZE // 2
            first_move = center * BOARD_SIZE + center
            board[first_move] = current
            current = WHITE
            
            move_count = 1
            
            while move_count < 60:
                nearby = get_nearby_moves(board, distance=2)
                if not nearby:
                    break
                
                with torch.no_grad():
                    board_tensor = torch.tensor(board, dtype=torch.long, device=self.device).unsqueeze(0)
                    turn_tensor = torch.tensor([0 if current == BLACK else 1], device=self.device)
                    
                    policy_logits, _ = self.model.forward_with_mask(
                        board_tensor, turn_tensor, legal_moves=[nearby]
                    )
                    
                    probs = F.softmax(policy_logits[0], dim=-1).cpu().numpy()
                    nearby_probs = [(pos, probs[pos]) for pos in nearby]
                    move = max(nearby_probs, key=lambda x: x[1])[0]
                
                board[move] = current
                move_count += 1
                
                if check_win(board, current):
                    wins[current] += 1
                    break
                
                current = 3 - current
            
            if move_count >= 60:
                draws += 1
        
        return {
            'black_wins': wins[BLACK],
            'white_wins': wins[WHITE],
            'draws': draws,
            'win_rate': (wins[BLACK] + wins[WHITE]) / num_games
        }

def load_all_dataset(filename="wuziqi_dataset_real.pkl"):
    """åŠ è½½å…¨éƒ¨æ•°æ®é›†"""
    print(f"åŠ è½½æ•°æ®é›†: {filename}")
    
    with open(filename, "rb") as f:
        raw_data = pickle.load(f)
    
    print(f"åŸå§‹æ•°æ®: {len(raw_data)} æ¡")
    
    all_samples = []
    
    for item in raw_data:
        try:
            if len(item) >= 6:
                board, action, value, fear_label, greed_label, scene_type = item[:6]
            else:
                board, action, value = item[:3]
                scene_type = 'normal'
            
            if scene_type == 'fear':
                reward = -0.2
            elif scene_type == 'greed':
                reward = 0.2
            elif scene_type == 'mixed':
                reward = 0.1
            else:
                reward = np.clip(value * 0.1, -0.1, 0.1) if value is not None else 0.0
            
            black_count = sum(1 for x in board if x == BLACK)
            white_count = sum(1 for x in board if x == WHITE)
            
            if black_count > white_count:
                turn = 1
            else:
                turn = 0
            
            all_samples.append({
                'board': board,
                'turn': turn,
                'move': action,
                'reward': reward,
                'scene_type': scene_type
            })
        except:
            continue
    
    print(f"æ€»æ ·æœ¬æ•°: {len(all_samples)}")
    
    counts = defaultdict(int)
    rewards = []
    for s in all_samples:
        counts[s['scene_type']] += 1
        rewards.append(s['reward'])
    
    for stype, cnt in counts.items():
        print(f"  {stype}: {cnt} ({cnt/len(all_samples):.1%})")
    
    print(f"  å¥–åŠ±èŒƒå›´: {min(rewards):.3f} ~ {max(rewards):.3f}")
    
    return all_samples

def main():
    print("=" * 70)
    print("äº”å­æ£‹è®­ç»ƒ Stage1 - åŠ è½½Stage0æ¨¡å‹ç»§ç»­è®­ç»ƒ")
    print("=" * 70)
    
    # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    device = get_best_device()
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    print("\n[1/4] åŠ è½½æ•°æ®é›†...")
    dataset = load_all_dataset("wuziqi_dataset_real.pkl")
    
    random.shuffle(dataset)
    split = int(len(dataset) * 0.9)
    train_data = dataset[:split]
    val_data = dataset[split:]
    
    print(f"\nè®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
    
    # åˆ›å»ºæ¨¡å‹
    print("\n[2/4] åˆå§‹åŒ–æ¨¡å‹...")
    model = FearGreedWuziqiModel(
        d_model=128,
        nhead=4,
        num_layers=2,
        dim_feedforward=256
    ).to(device)
    
    # å°è¯•åŠ è½½Stage0æ¨¡å‹
    try:
        # åŠ è½½æ—¶å…ˆæ”¾åˆ°CPUï¼Œå†ç§»åˆ°ç›®æ ‡è®¾å¤‡
        state_dict = torch.load("wuziqi_stage0_final.pth", map_location='cpu')
        model.load_state_dict(state_dict)
        model = model.to(device)
        print("âœ… æˆåŠŸåŠ è½½ Stage0 æ¨¡å‹")
    except Exception as e:
        print(f"âš ï¸ åŠ è½½å¤±è´¥: {e}, ä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Stage1Trainer(model, device, lr=2e-5)
    
    # è®­ç»ƒ
    print("\n[3/4] å¼€å§‹å¾®è°ƒ...")
    print("-" * 90)
    
    best_win_rate = 0
    
    for epoch in range(1, 21):
        train_stats = trainer.train_epoch(train_data, batch_size=32)
        
        if epoch % 2 == 0:
            eval_results = trainer.evaluate(num_games=5)
            
            print(f"\nEpoch {epoch:3d} | Loss: {train_stats.get('total_loss', 0):.4f}")
            print(f"  å¯¹æˆ˜: é»‘èƒœ={eval_results['black_wins']}, "
                  f"ç™½èƒœ={eval_results['white_wins']}, "
                  f"å¹³å±€={eval_results['draws']}, "
                  f"èƒœç‡={eval_results['win_rate']:.2%}")
            
            if eval_results['win_rate'] > best_win_rate:
                best_win_rate = eval_results['win_rate']
                # ä¿å­˜æ—¶è½¬ä¸ºCPUå¼ é‡
                torch.save(model.cpu().state_dict(), f"wuziqi_stage1_best_{best_win_rate:.0%}_epoch{epoch}.pth")
                model.to(device)  # ç§»å›åŸè®¾å¤‡
                print(f"          ğŸ† æ–°æœ€ä½³æ¨¡å‹! èƒœç‡={best_win_rate:.2%}")
        
        trainer.scheduler.step()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.cpu().state_dict(), "wuziqi_stage1_final.pth")
    
    print("\n[4/4] è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    print(f"Stage1æœ€ä½³èƒœç‡: {best_win_rate:.2%}")
    print("=" * 70)

if __name__ == "__main__":
    main()