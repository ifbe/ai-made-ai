# train_stage1_minimax_winner.py
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pickle
import random
import numpy as np
import glob
import os
import json
import time
import argparse
from collections import defaultdict
from datetime import datetime
from model import FearGreedWuziqiModel
from game import (
    check_win, get_legal_moves, get_nearby_moves,
    BOARD_SIZE, BLACK, WHITE, EMPTY, BOARD_POSITIONS, pos_to_str
)

# åˆ›å»ºä¿å­˜æ¨¡å‹çš„æ–‡ä»¶å¤¹
os.makedirs("stage1", exist_ok=True)

# è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
def get_best_device():
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡ï¼šMLX (Apple Silicon) > CUDA > CPU"""
    try:
        if torch.backends.mps.is_available():
            print("âœ… ä½¿ç”¨ Apple Silicon (MPS)")
            return torch.device("mps")
    except:
        pass
    
    if torch.cuda.is_available():
        try:
            test_tensor = torch.tensor([1.0]).cuda()
            print(f"âœ… ä½¿ç”¨ NVIDIA GPU (CUDA) - {torch.cuda.get_device_name(0)}")
            return torch.device("cuda")
        except:
            print("âš ï¸ CUDA å¯ç”¨ä½†å…¼å®¹æ€§æœ‰é—®é¢˜ï¼Œå›é€€åˆ° CPU")
    
    print("âš ï¸ ä½¿ç”¨ CPU")
    return torch.device("cpu")

def find_best_stage0_model():
    """æŸ¥æ‰¾stage0çš„æœ€ä½³æ¨¡å‹ï¼ˆæŒ‰å‡†ç¡®ç‡ï¼‰"""
    model_files = glob.glob("stage0/wuziqi_stage0_best_*.pth")
    
    if not model_files:
        model_files = glob.glob("stage0/wuziqi_stage0_final.pth")
        if model_files:
            return model_files[0]
        return None
    
    best_model = None
    best_acc = 0
    for f in model_files:
        try:
            acc_str = f.split('best_')[1].split('%')[0]
            acc = float(acc_str)
            if acc > best_acc:
                best_acc = acc
                best_model = f
        except:
            continue
    
    if best_model:
        print(f"   æ‰¾åˆ°æœ€ä½³Stage0æ¨¡å‹: {best_model} (å‡†ç¡®ç‡ {best_acc}%)")
        return best_model
    
    return max(model_files, key=os.path.getctime) if model_files else None

def find_latest_checkpoint():
    """æŸ¥æ‰¾æœ€æ–°çš„stage1æ£€æŸ¥ç‚¹"""
    checkpoint_files = glob.glob("stage1/wuziqi_stage1_winner_checkpoint_epoch*.pth")
    if not checkpoint_files:
        return None
    
    epochs = []
    for f in checkpoint_files:
        try:
            epoch = int(f.split('epoch')[1].split('.')[0])
            epochs.append((epoch, f))
        except:
            continue
    
    if epochs:
        latest = max(epochs, key=lambda x: x[0])
        return latest[1]
    return None

def print_game_board(moves_history, title=None):
    """
    æ‰“å°ä¸€å±€æ£‹çš„å®Œæ•´æ£‹ç›˜ï¼Œç”¨æ•°å­—æ˜¾ç¤ºè½å­é¡ºåº
    é»‘å­ï¼šç™½è‰²æ•°å­—åœ¨é»‘è‰²èƒŒæ™¯
    ç™½å­ï¼šé»‘è‰²æ•°å­—åœ¨ç™½è‰²èƒŒæ™¯
    """
    if title:
        print(f"\n{title}")
    
    board_state = [EMPTY] * BOARD_POSITIONS
    move_numbers = [0] * BOARD_POSITIONS
    
    for step, (board, player, move) in enumerate(moves_history):
        board_state[move] = player
        move_numbers[move] = step + 1
    
    print("\n   ", end="")
    for i in range(BOARD_SIZE):
        if i < 10:
            print(f"â”‚ {i} ", end="")
        else:
            print(f"â”‚ {chr(ord('a') + (i-10))} ", end="")
    print("â”‚")
    
    print("   " + "â”œâ”€â”€â”€" * BOARD_SIZE + "â”¤")
    
    for i in range(BOARD_SIZE):
        if i < 10:
            print(f" {i} ", end="")
        else:
            print(f" {chr(ord('a') + (i-10))} ", end="")
        
        for j in range(BOARD_SIZE):
            idx = i * BOARD_SIZE + j
            player = board_state[idx]
            move_num = move_numbers[idx]
            
            if player == BLACK:
                print(f"â”‚\033[97;40m{move_num:^3}\033[0m", end="")
            elif player == WHITE:
                print(f"â”‚\033[30;107m{move_num:^3}\033[0m", end="")
            else:
                print(f"â”‚   ", end="")
        
        print("â”‚")
        
        if i < BOARD_SIZE - 1:
            print("   " + "â”œâ”€â”€â”€" * BOARD_SIZE + "â”¤")
    
    print("   " + "â””â”€â”€â”€" * BOARD_SIZE + "â”˜")
    
    print("\nå›¾ä¾‹: \033[97;40m æ•°å­— \033[0m = é»‘å­, \033[30;107m æ•°å­— \033[0m = ç™½å­")

def save_games_to_file(games_list, filename="history_stage1_winner.json"):
    """æ‰¹é‡ä¿å­˜æ£‹å±€åˆ°æ–‡ä»¶"""
    all_games = []
    
    if os.path.exists(filename):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                all_games = json.load(f)
        except:
            all_games = []
    
    all_games.extend(games_list)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(all_games, f, ensure_ascii=False, indent=2)

# ============ ç®€åŒ–åŠ é€Ÿç‰ˆ Minimax ============
def quick_evaluate(board):
    """ä¼˜åŒ–ç‰ˆè¯„ä¼°å‡½æ•°ï¼šåªåœ¨æ£‹å­å‘¨å›´æ£€æŸ¥å¨èƒ"""
    if check_win(board, BLACK):
        return 10000
    if check_win(board, WHITE):
        return -10000
    
    score = 0
    directions = [(1, 0), (0, 1), (1, 1), (1, -1)]
    
    # æ‰¾å‡ºæ‰€æœ‰æ£‹å­çš„ä½ç½®
    stones = [i for i in range(BOARD_POSITIONS) if board[i] != EMPTY]
    if not stones:
        return 0
    
    # åªæ£€æŸ¥æ£‹å­å‘¨å›´2æ ¼å†…çš„ä½ç½®ï¼ˆå‡å°‘æ£€æŸ¥èŒƒå›´ï¼‰
    check_positions = set()
    threat_check_positions = set()  # ä¸“é—¨ç”¨äºå¨èƒæ£€æŸ¥
    
    for pos in stones:
        r, c = pos // BOARD_SIZE, pos % BOARD_SIZE
        # æ£‹å‹æ£€æŸ¥èŒƒå›´ï¼ˆ3æ ¼ï¼‰
        for dr in range(-3, 4):
            for dc in range(-3, 4):
                nr, nc = r + dr, c + dc
                if 0 <= nr < BOARD_SIZE and 0 <= nc < BOARD_SIZE:
                    check_positions.add(nr * BOARD_SIZE + nc)
        
        # å¨èƒæ£€æŸ¥èŒƒå›´ï¼ˆ2æ ¼ï¼Œæ›´å¿«ï¼‰
        for dr in range(-2, 3):
            for dc in range(-2, 3):
                nr, nc = r + dr, c + dc
                if 0 <= nr < BOARD_SIZE and 0 <= nc < BOARD_SIZE:
                    threat_check_positions.add(nr * BOARD_SIZE + nc)
    
    # åªæ£€æŸ¥æ£‹å­å‘¨å›´çš„ç©ºä½æ˜¯å¦æœ‰å¨èƒ
    black_threats = 0
    white_threats = 0
    
    for pos in threat_check_positions:
        if board[pos] != EMPTY:
            continue
        
        # æ£€æŸ¥é»‘æ£‹ä¸‹è¿™é‡Œæ˜¯å¦èƒ½èµ¢
        board[pos] = BLACK
        if check_win(board, BLACK):
            black_threats += 1
        board[pos] = EMPTY
        
        # æ£€æŸ¥ç™½æ£‹ä¸‹è¿™é‡Œæ˜¯å¦èƒ½èµ¢
        board[pos] = WHITE
        if check_win(board, WHITE):
            white_threats += 1
        board[pos] = EMPTY
    
    # å¨èƒæƒ©ç½š
    if black_threats > 0:
        score -= 5000 * black_threats
    if white_threats > 0:
        score += 5000 * white_threats
    
    # æ£‹å‹è¯„åˆ†ï¼ˆåªæ£€æŸ¥æ£‹å­å‘¨å›´ï¼‰
    for i in check_positions:
        if board[i] == EMPTY:
            continue
        
        r, c = i // BOARD_SIZE, i % BOARD_SIZE
        current_player = board[i]
        
        for dr, dc in directions:
            count = 1
            # æ­£æ–¹å‘
            for step in range(1, 5):
                nr, nc = r + dr * step, c + dc * step
                if nr < 0 or nr >= BOARD_SIZE or nc < 0 or nc >= BOARD_SIZE:
                    break
                if board[nr * BOARD_SIZE + nc] == current_player:
                    count += 1
                else:
                    break
            # åæ–¹å‘
            for step in range(1, 5):
                nr, nc = r - dr * step, c - dc * step
                if nr < 0 or nr >= BOARD_SIZE or nc < 0 or nc >= BOARD_SIZE:
                    break
                if board[nr * BOARD_SIZE + nc] == current_player:
                    count += 1
                else:
                    break
            
            # è¿›æ”»åˆ†
            multiplier = 1 if current_player == BLACK else -1
            if count >= 5:
                score += multiplier * 10000
            elif count == 4:
                score += multiplier * 1000
            elif count == 3:
                score += multiplier * 100
            elif count == 2:
                score += multiplier * 10
    
    return score

def get_noisy_minimax_move(board, player, depth=2, temperature=0.3):
    """å¸¦å™ªå£°çš„Minimaxèµ°æ³•ï¼Œå¢åŠ å¤šæ ·æ€§"""
    # è·å–æ‰€æœ‰åˆæ³•èµ°æ³•
    legals = get_nearby_moves(board, distance=2)
    if not legals:
        legals = get_legal_moves(board)
    
    if not legals:
        return None
    
    # å¦‚æœæ·±åº¦ä¸º0æˆ–æ£‹ç›˜å¿«æ»¡äº†ï¼Œéšæœºé€‰æ‹©
    if depth == 0 or len(legals) < 3:
        return random.choice(legals)
    
    maximizing = (player == BLACK)
    
    # å¿«é€Ÿè¯„ä¼°æ¯ä¸ªèµ°æ³•
    move_scores = []
    for move in legals:
        board[move] = player
        score = quick_evaluate(board)
        board[move] = EMPTY
        move_scores.append(score)
    
    # è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆæ¸©åº¦å‚æ•°æ§åˆ¶éšæœºæ€§ï¼‰
    scores = np.array(move_scores)
    
    # å½’ä¸€åŒ–åˆ†æ•°åˆ°åˆç†èŒƒå›´ï¼Œé¿å…expæº¢å‡º
    scores = (scores - scores.mean()) / (scores.std() + 1e-8)
    scores = np.clip(scores, -10, 10)
    
    if maximizing:
        # æœ€å¤§åŒ–ç©å®¶ï¼šåˆ†æ•°è¶Šé«˜æ¦‚ç‡è¶Šå¤§
        probs = np.exp(scores / temperature)
    else:
        # æœ€å°åŒ–ç©å®¶ï¼šåˆ†æ•°è¶Šä½æ¦‚ç‡è¶Šå¤§
        probs = np.exp(-scores / temperature)
    
    probs = probs / probs.sum()
    
    # æŒ‰æ¦‚ç‡é€‰æ‹©
    return np.random.choice(legals, p=probs)

# ============ èµ¢å®¶å­¦ä¹  + è¾“å®¶æƒ©ç½šè®­ç»ƒå™¨ï¼ˆå¸¦éšæœºæ€§ï¼‰ ============

class WinnerTrainer:
    def __init__(self, model, device, lr=1e-4, start_epoch=1):
        self.model = model
        self.device = device
        self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=50)
        self.start_epoch = start_epoch
        self.best_acc = 0
        
        if device.type == 'mps':
            print("   MPS æ¨¡å¼: ä½¿ç”¨æ›´ç¨³å®šçš„è®¾ç½®")
            torch.backends.mps.enable_fallback_to_cpu = True
    
    def save_checkpoint(self, epoch, filename):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.cpu().state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_acc': self.best_acc
        }
        full_path = os.path.join("stage1", filename)
        torch.save(checkpoint, full_path)
        self.model.to(self.device)
        print(f"   ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {full_path}")
    
    def load_checkpoint(self, filename):
        if os.path.exists(filename):
            checkpoint = torch.load(filename, map_location='cpu')
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model = self.model.to(self.device)
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            self.start_epoch = checkpoint['epoch'] + 1
            self.best_acc = checkpoint.get('best_acc', 0)
            print(f"   âœ… ä» {filename} æ¢å¤è®­ç»ƒ")
            print(f"      ä¸Šæ¬¡è®­ç»ƒåˆ° Epoch {checkpoint['epoch']}, æœ€ä½³å‡†ç¡®ç‡: {self.best_acc:.2%}")
            return True
        return False
    
    def generate_games(self, num_games=10, depth1=2, depth2=1, save_games=True):
        """
        è®©ä¸¤ä¸ªä¸åŒæ·±åº¦çš„Minimaxå¯¹å¼ˆï¼ˆå¸¦éšæœºæ€§ï¼‰
        depth1: ç¬¬ä¸€ä¸ªæ·±åº¦
        depth2: ç¬¬äºŒä¸ªæ·±åº¦
        è°èµ¢å°±å­¦è°çš„èµ°æ³•ï¼ˆæ­£å¥–åŠ±ï¼‰ï¼Œè°è¾“å°±æƒ©ç½šè°çš„èµ°æ³•ï¼ˆè´Ÿå¥–åŠ±ï¼‰
        """
        training_data = []  # æ¯ä¸ªå…ƒç´ : {'board': board, 'player': player, 'move': move, 'reward': reward}
        games_records = []
        win_count_depth1 = 0
        win_count_depth2 = 0
        draw_count = 0
        first_game_board = None
        first_game_moves = 0
        first_game_winner = None
        total_moves = 0
        
        for game_idx in range(num_games):
            # éšæœºå†³å®šè°å…ˆæ‰‹
            depth1_first = random.choice([True, False])
            
            board = [EMPTY] * BOARD_POSITIONS
            current = BLACK
            game_steps = []
            moves_history = []
            
            # ç¬¬ä¸€æ­¥éšæœºåŒ–ï¼ˆä¸ä¸€å®šæ˜¯ä¸­å¿ƒï¼‰
            center = BOARD_SIZE // 2
            if random.random() < 0.7:  # 70%æ¦‚ç‡ä¸‹åœ¨ä¸­å¿ƒé™„è¿‘éšæœºä½ç½®
                all_moves = get_nearby_moves(board, distance=3)
                if all_moves:
                    first_move = random.choice(all_moves)
                else:
                    first_move = center * BOARD_SIZE + center
            else:
                first_move = center * BOARD_SIZE + center
            
            board[first_move] = current
            game_steps.append((board.copy(), current, first_move))
            moves_history.append((board.copy(), current, first_move))
            current = WHITE
            move_count = 1
            
            while move_count < 60:
                is_depth1_turn = (current == BLACK and depth1_first) or (current == WHITE and not depth1_first)
                
                if is_depth1_turn:
                    # æ·±åº¦1çš„ç©å®¶ï¼Œç”¨ç¨é«˜çš„æ¸©åº¦å¢åŠ éšæœºæ€§
                    move = get_noisy_minimax_move(board, current, depth=depth1, temperature=0.4)
                else:
                    # æ·±åº¦2çš„ç©å®¶ï¼Œç”¨ç¨ä½çš„æ¸©åº¦ä¿æŒä¸€å®šæ°´å¹³
                    move = get_noisy_minimax_move(board, current, depth=depth2, temperature=0.3)
                
                if move is None:
                    break
                
                board[move] = current
                game_steps.append((board.copy(), current, move))
                moves_history.append((board.copy(), current, move))
                move_count += 1
                
                if check_win(board, current):
                    break
                
                current = 3 - current
            
            # ç¡®å®šèƒœè´Ÿ
            if check_win(board, BLACK):
                winner = BLACK
                winner_str = "black"
                winner_cn = "é»‘æ£‹"
                if depth1_first:
                    win_count_depth1 += 1
                else:
                    win_count_depth2 += 1
            elif check_win(board, WHITE):
                winner = WHITE
                winner_str = "white"
                winner_cn = "ç™½æ£‹"
                if not depth1_first:
                    win_count_depth1 += 1
                else:
                    win_count_depth2 += 1
            else:
                winner = None
                winner_str = "draw"
                winner_cn = "å¹³å±€"
                draw_count += 1
            
            # åˆ†é…å¥–åŠ±
            gamma = 0.95
            T = len(game_steps)
            
            if winner is not None:
                for t, (board_state, player, move) in enumerate(game_steps):
                    steps_to_end = T - t
                    if player == winner:
                        # èµ¢å®¶çš„èµ°æ³•å¾—æ­£å¥–åŠ±
                        reward = 1.0 * (gamma ** steps_to_end)
                    else:
                        # è¾“å®¶çš„èµ°æ³•å¾—è´Ÿå¥–åŠ±
                        reward = -0.5 * (gamma ** steps_to_end)
                    
                    training_data.append({
                        'board': board_state,
                        'player': 0 if player == BLACK else 1,
                        'move': move,
                        'reward': reward
                    })
                    total_moves += 1
            
            # ä¿å­˜ç¬¬ä¸€å±€ç”¨äºæ‰“å°
            if game_idx == 0:
                first_game_board = moves_history
                first_game_moves = move_count
                first_game_winner = winner_cn
            
            # ä¿å­˜æ£‹å±€è®°å½•
            if save_games:
                games_records.append({
                    'timestamp': datetime.now().isoformat(),
                    'epoch': self.start_epoch,
                    'game_id': game_idx,
                    'depth1_first': depth1_first,
                    'depth1': depth1,
                    'depth2': depth2,
                    'winner': winner_str,
                    'moves': [(pos_to_str(move), 'black' if player == BLACK else 'white') 
                             for _, player, move in moves_history]
                })
        
        if save_games and games_records:
            save_games_to_file(games_records)
        
        print(f"   å¯¹å±€ç»“æœ: æ·±åº¦{depth1}èƒœ={win_count_depth1}, æ·±åº¦{depth2}èƒœ={win_count_depth2}, å¹³å±€={draw_count}")
        print(f"   æ”¶é›†åˆ° {total_moves} ä¸ªè®­ç»ƒæ ·æœ¬")
        
        return training_data, first_game_board, first_game_moves, first_game_winner
    
    def train_step(self, batch):
        boards = []
        players = []
        actions = []
        rewards = []
        legal_moves_list = []
        
        for item in batch:
            board = item['board']
            player = item['player']
            action = item['move']
            reward = item['reward']
            
            boards.append(board)
            players.append(player)
            actions.append(action)
            rewards.append(reward)
            
            nearby = get_nearby_moves(board, distance=2)
            if not nearby:
                center = BOARD_SIZE // 2
                nearby = [center * BOARD_SIZE + center]
            legal_moves_list.append(nearby)
        
        boards = torch.tensor(boards, dtype=torch.long, device=self.device)
        players = torch.tensor(players, dtype=torch.long, device=self.device)
        actions = torch.tensor(actions, dtype=torch.long, device=self.device)
        rewards = torch.tensor(rewards, dtype=torch.float, device=self.device)
        
        # å‰å‘ä¼ æ’­
        policy_logits, values = self.model.forward_with_mask(
            boards, players, legal_moves=legal_moves_list
        )
        
        policy_logits = torch.clamp(policy_logits, -20, 20)
        
        # ç­–ç•¥æ¢¯åº¦æŸå¤±
        log_probs = F.log_softmax(policy_logits, dim=-1)
        action_log_probs = log_probs.gather(1, actions.unsqueeze(1)).squeeze()
        
        # ä¼˜åŠ¿å‡½æ•°ï¼šreward - baseline (values)
        advantages = rewards - values.squeeze()
        advantages = torch.clamp(advantages, -1.0, 1.0)
        
        policy_loss = -(action_log_probs * advantages.detach()).mean()
        
        # ä»·å€¼æŸå¤±ï¼ˆè®©valuesæ‹Ÿåˆrewardsï¼‰
        value_loss = F.mse_loss(values.squeeze(-1), rewards)
        
        # ç†µæ­£åˆ™åŒ–ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        entropy = 0
        for b in range(len(batch)):
            valid_indices = legal_moves_list[b]
            if valid_indices:
                valid_logits = policy_logits[b][valid_indices]
                valid_probs = F.softmax(valid_logits, dim=-1)
                valid_log_probs = F.log_softmax(valid_logits, dim=-1)
                entropy += -(valid_probs * valid_log_probs).sum()
        entropy = entropy / len(batch)
        entropy_bonus = 0.0002 * entropy
        
        total_loss = policy_loss + 0.5 * value_loss - entropy_bonus
        
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)
        self.optimizer.step()
        
        return {
            'total_loss': total_loss.item(),
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'entropy': entropy.item(),
            'avg_reward': rewards.mean().item()
        }
    
    def train_epoch(self, games_per_epoch=10, depth1=2, depth2=1, batch_size=32):
        """æ¯ä¸ªepochï¼šè®©ä¸¤ä¸ªMinimaxå¯¹å¼ˆï¼Œæ”¶é›†æ•°æ®å¹¶è®­ç»ƒ"""
        self.model.train()
        
        training_data, first_board, first_moves, first_winner = self.generate_games(
            num_games=games_per_epoch, 
            depth1=depth1,
            depth2=depth2,
            save_games=True
        )
        
        # æ‰“å°ç¬¬ä¸€å±€æ£‹è°±
        if first_board:
            title = f"å¯¹å¼ˆæ ·ä¾‹ - ç»“æœ: {first_winner} (æ·±åº¦{depth1} vs æ·±åº¦{depth2}, å…±{first_moves}æ­¥)"
            print_game_board(first_board, title)
            print()
        
        if len(training_data) == 0:
            print("   è­¦å‘Šï¼šæœ¬è½®æ²¡æœ‰æ”¶é›†åˆ°è®­ç»ƒæ•°æ®")
            return {
                'total_loss': 0.0,
                'policy_loss': 0.0,
                'value_loss': 0.0,
                'entropy': 0.0,
                'avg_reward': 0.0
            }
        
        random.shuffle(training_data)
        
        total_loss = 0.0
        total_policy_loss = 0.0
        total_value_loss = 0.0
        total_entropy = 0.0
        total_avg_reward = 0.0
        batch_count = 0
        
        for start_idx in range(0, len(training_data), batch_size):
            batch = training_data[start_idx:start_idx + batch_size]
            stats = self.train_step(batch)
            
            total_loss += stats['total_loss']
            total_policy_loss += stats['policy_loss']
            total_value_loss += stats['value_loss']
            total_entropy += stats['entropy']
            total_avg_reward += stats['avg_reward']
            batch_count += 1
        
        avg_stats = {
            'total_loss': total_loss / batch_count if batch_count > 0 else 0.0,
            'policy_loss': total_policy_loss / batch_count if batch_count > 0 else 0.0,
            'value_loss': total_value_loss / batch_count if batch_count > 0 else 0.0,
            'entropy': total_entropy / batch_count if batch_count > 0 else 0.0,
            'avg_reward': total_avg_reward / batch_count if batch_count > 0 else 0.0
        }
        
        print(f"   è®­ç»ƒç»Ÿè®¡: Loss={avg_stats['total_loss']:.4f} "
              f"(Policy={avg_stats['policy_loss']:.4f}, "
              f"Value={avg_stats['value_loss']:.4f}, "
              f"Entropy={avg_stats['entropy']:.4f})")
        print(f"   å¹³å‡å¥–åŠ±: {avg_stats['avg_reward']:.4f}")
        
        return avg_stats

def main():
    parser = argparse.ArgumentParser(description='äº”å­æ£‹ Stage1 - èµ¢å®¶å­¦ä¹  + è¾“å®¶æƒ©ç½šï¼ˆå¸¦éšæœºæ€§ï¼‰')
    parser.add_argument('--new', action='store_true', 
                       help='ä»å¤´å¼€å§‹è®­ç»ƒï¼ˆé»˜è®¤æ˜¯ç»§ç»­ä¸Šæ¬¡çš„è®­ç»ƒï¼‰')
    parser.add_argument('--checkpoint', '-ckpt', type=str, default=None,
                       help='æŒ‡å®šæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--epochs', '-e', type=int, default=50,
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤: 50)')
    parser.add_argument('--games', '-g', type=int, default=10,
                       help='æ¯è½®å¯¹å±€æ•° (é»˜è®¤: 10)')
    parser.add_argument('--depth1', type=int, default=2,
                       help='ç¬¬ä¸€ä¸ªæ·±åº¦ (é»˜è®¤: 2)')
    parser.add_argument('--depth2', type=int, default=2,
                       help='ç¬¬äºŒä¸ªæ·±åº¦ (é»˜è®¤: 2)')
    args = parser.parse_args()
    
    print("=" * 70)
    print("äº”å­æ£‹è®­ç»ƒ Stage1 - èµ¢å®¶å­¦ä¹  + è¾“å®¶æƒ©ç½šï¼ˆå¸¦éšæœºæ€§ï¼‰")
    print("=" * 70)
    print(f"æ·±åº¦{args.depth1} vs æ·±åº¦{args.depth2}")
    
    device = get_best_device()
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    print("\n[1/3] åˆå§‹åŒ–æ¨¡å‹...")
    model = FearGreedWuziqiModel(
        d_model=128,
        nhead=4,
        num_layers=2,
        dim_feedforward=256
    )
    
    trainer = WinnerTrainer(model, device, lr=1e-4)
    
    if not args.new:
        checkpoint_file = None
        if args.checkpoint:
            checkpoint_file = args.checkpoint
        else:
            checkpoint_file = find_latest_checkpoint()
        
        if checkpoint_file and os.path.exists(checkpoint_file):
            trainer.load_checkpoint(checkpoint_file)
            print("âœ… ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ")
        else:
            stage0_model = find_best_stage0_model()
            if stage0_model:
                try:
                    checkpoint = torch.load(stage0_model, map_location='cpu')
                    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['model_state_dict'])
                    else:
                        model.load_state_dict(checkpoint)
                    model = model.to(device)
                    print(f"âœ… æˆåŠŸåŠ è½½ Stage0 æ¨¡å‹: {stage0_model}")
                except Exception as e:
                    print(f"âš ï¸ Stage0æ¨¡å‹åŠ è½½å¤±è´¥: {e}, ä½¿ç”¨éšæœºåˆå§‹åŒ–")
                    model = model.to(device)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°Stage0æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                model = model.to(device)
    else:
        print("ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒ")
        stage0_model = find_best_stage0_model()
        if stage0_model:
            try:
                checkpoint = torch.load(stage0_model, map_location='cpu')
                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                model = model.to(device)
                print(f"âœ… åŠ è½½ Stage0 æ¨¡å‹ä½œä¸ºåˆå§‹æƒé‡: {stage0_model}")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å¤±è´¥: {e}, ä½¿ç”¨éšæœºåˆå§‹åŒ–")
                model = model.to(device)
        else:
            print("âš ï¸ æœªæ‰¾åˆ°Stage0æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
            model = model.to(device)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    print("\n[2/3] å¼€å§‹å¯¹å¼ˆè®­ç»ƒ...")
    print("-" * 90)
    
    for epoch in range(trainer.start_epoch, args.epochs + 1):
        print(f"\nEpoch {epoch}/{args.epochs} - å¯¹å¼ˆ (æ·±åº¦{args.depth1} vs æ·±åº¦{args.depth2})...")
        train_stats = trainer.train_epoch(
            games_per_epoch=args.games, 
            depth1=args.depth1,
            depth2=args.depth2,
            batch_size=32
        )
        
        print(f"\nEpoch {epoch:3d} | Total Loss: {train_stats['total_loss']:.4f} | Policy: {train_stats['policy_loss']:.4f} | Value: {train_stats['value_loss']:.4f} | Reward: {train_stats['avg_reward']:.4f}")
        
        trainer.save_checkpoint(epoch, f"wuziqi_stage1_winner_checkpoint_epoch{epoch}.pth")
        trainer.scheduler.step()
    
    print("\n[3/3] è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)

if __name__ == "__main__":
    main()
