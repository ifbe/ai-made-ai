# train_stage2_selfplay.py
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import pickle
import random
import numpy as np
import glob
import os
import json
import time
import argparse
from collections import defaultdict
from datetime import datetime
from model import FearGreedWuziqiModel
from game import (
    check_win, get_legal_moves, get_nearby_moves,
    BOARD_SIZE, BLACK, WHITE, EMPTY, BOARD_POSITIONS, pos_to_str
)

# åˆ›å»ºä¿å­˜æ¨¡å‹çš„æ–‡ä»¶å¤¹
os.makedirs("stage2", exist_ok=True)

# è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
def get_best_device():
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡ï¼šMLX (Apple Silicon) > CUDA > CPU"""
    try:
        if torch.backends.mps.is_available():
            print("âœ… ä½¿ç”¨ Apple Silicon (MPS)")
            return torch.device("mps")
    except:
        pass
    
    if torch.cuda.is_available():
        try:
            test_tensor = torch.tensor([1.0]).cuda()
            print(f"âœ… ä½¿ç”¨ NVIDIA GPU (CUDA) - {torch.cuda.get_device_name(0)}")
            return torch.device("cuda")
        except:
            print("âš ï¸ CUDA å¯ç”¨ä½†å…¼å®¹æ€§æœ‰é—®é¢˜ï¼Œå›é€€åˆ° CPU")
    
    print("âš ï¸ ä½¿ç”¨ CPU")
    return torch.device("cpu")

def find_best_stage1_model():
    """æŸ¥æ‰¾stage1çš„æœ€ä½³æ¨¡å‹ï¼ˆå–epochæœ€å¤§çš„ï¼‰"""
    # ä¼˜å…ˆæ‰¾stage1çš„checkpoint
    checkpoint_files = glob.glob("stage1/wuziqi_stage1_minimax_checkpoint_epoch*.pth")
    if checkpoint_files:
        best_model = None
        max_epoch = 0
        for f in checkpoint_files:
            try:
                epoch_str = f.split('epoch')[1].split('.')[0]
                epoch = int(epoch_str)
                if epoch > max_epoch:
                    max_epoch = epoch
                    best_model = f
            except:
                continue
        if best_model:
            print(f"   æ‰¾åˆ°Stage1æ¨¡å‹: epoch {max_epoch}")
            return best_model
    
    # å¦‚æœæ²¡æœ‰ï¼Œæ‰¾stage1çš„bestæ–‡ä»¶
    best_files = glob.glob("stage1/wuziqi_stage1_minimax_best_*.pth")
    if best_files:
        return max(best_files, key=os.path.getctime)
    
    return None

def find_latest_checkpoint():
    """æŸ¥æ‰¾æœ€æ–°çš„stage2æ£€æŸ¥ç‚¹"""
    checkpoint_files = glob.glob("stage2/wuziqi_stage2_checkpoint_epoch*.pth")
    if not checkpoint_files:
        return None
    
    epochs = []
    for f in checkpoint_files:
        try:
            epoch = int(f.split('epoch')[1].split('.')[0])
            epochs.append((epoch, f))
        except:
            continue
    
    if epochs:
        latest = max(epochs, key=lambda x: x[0])
        return latest[1]
    return None

def print_game_board(moves_history, title=None):
    """
    æ‰“å°ä¸€å±€æ£‹çš„å®Œæ•´æ£‹ç›˜ï¼Œç”¨æ•°å­—æ˜¾ç¤ºè½å­é¡ºåº
    é»‘å­ï¼šç™½è‰²æ•°å­—åœ¨é»‘è‰²èƒŒæ™¯
    ç™½å­ï¼šé»‘è‰²æ•°å­—åœ¨ç™½è‰²èƒŒæ™¯
    """
    if title:
        print(f"\n{title}")
    
    board_state = [EMPTY] * BOARD_POSITIONS
    move_numbers = [0] * BOARD_POSITIONS
    
    for step, (board, player, move) in enumerate(moves_history):
        board_state[move] = player
        move_numbers[move] = step + 1
    
    print("\n   ", end="")
    for i in range(BOARD_SIZE):
        if i < 10:
            print(f"â”‚ {i} ", end="")
        else:
            print(f"â”‚ {chr(ord('a') + (i-10))} ", end="")
    print("â”‚")
    
    print("   " + "â”œâ”€â”€â”€" * BOARD_SIZE + "â”¤")
    
    for i in range(BOARD_SIZE):
        if i < 10:
            print(f" {i} ", end="")
        else:
            print(f" {chr(ord('a') + (i-10))} ", end="")
        
        for j in range(BOARD_SIZE):
            idx = i * BOARD_SIZE + j
            player = board_state[idx]
            move_num = move_numbers[idx]
            
            if player == BLACK:
                print(f"â”‚\033[97;40m{move_num:^3}\033[0m", end="")
            elif player == WHITE:
                print(f"â”‚\033[30;107m{move_num:^3}\033[0m", end="")
            else:
                print(f"â”‚   ", end="")
        
        print("â”‚")
        
        if i < BOARD_SIZE - 1:
            print("   " + "â”œâ”€â”€â”€" * BOARD_SIZE + "â”¤")
    
    print("   " + "â””â”€â”€â”€" * BOARD_SIZE + "â”˜")
    
    print("\nå›¾ä¾‹: \033[97;40m æ•°å­— \033[0m = é»‘å­, \033[30;107m æ•°å­— \033[0m = ç™½å­")

def save_games_to_file(games_list, filename="history_stage2_selfplay.json"):
    """æ‰¹é‡ä¿å­˜æ£‹å±€åˆ°æ–‡ä»¶"""
    all_games = []
    
    if os.path.exists(filename):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                all_games = json.load(f)
        except:
            all_games = []
    
    all_games.extend(games_list)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(all_games, f, ensure_ascii=False, indent=2)

class Stage2SelfPlayTrainer:
    def __init__(self, model, device, lr=1e-5, start_epoch=1):
        self.model = model
        self.device = device
        self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=50)
        self.start_epoch = start_epoch
        self.best_win_rate = 0
        
        if device.type == 'mps':
            print("   MPS æ¨¡å¼: ä½¿ç”¨æ›´ç¨³å®šçš„è®¾ç½®")
            torch.backends.mps.enable_fallback_to_cpu = True
    
    def save_checkpoint(self, epoch, filename):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.cpu().state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_win_rate': self.best_win_rate
        }
        full_path = os.path.join("stage2", filename)
        torch.save(checkpoint, full_path)
        self.model.to(self.device)
        print(f"   ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {full_path}")
    
    def load_checkpoint(self, filename):
        if os.path.exists(filename):
            checkpoint = torch.load(filename, map_location='cpu')
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model = self.model.to(self.device)
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            self.start_epoch = checkpoint['epoch'] + 1
            self.best_win_rate = checkpoint.get('best_win_rate', 0)
            print(f"   âœ… ä» {filename} æ¢å¤è®­ç»ƒ")
            print(f"      ä¸Šæ¬¡è®­ç»ƒåˆ° Epoch {checkpoint['epoch']}, æœ€ä½³èƒœç‡: {self.best_win_rate:.2%}")
            return True
        return False
    
    def generate_self_play_games(self, num_games=20, max_moves=60, save_games=True):
        """
        è‡ªæˆ‘å¯¹å¼ˆç”Ÿæˆè®­ç»ƒæ•°æ®
        æ¯å±€éšæœºå†³å®šå…ˆæ‰‹æ–¹ï¼Œè®©æ¨¡å‹åŒæ—¶å­¦ä¹ å…ˆæ‰‹å’Œåæ‰‹ç­–ç•¥
        """
        games_data = []
        games_records = []
        black_win_count = 0
        white_win_count = 0
        draw_count = 0
        first_game_board = None
        first_game_moves = 0
        first_game_winner = None
        
        for game_idx in range(num_games):
            # éšæœºå†³å®šæœ¬å±€è°å…ˆæ‰‹
            black_first = random.choice([True, False])  # True=é»‘å…ˆ, False=ç™½å…ˆ
            
            board = [EMPTY] * BOARD_POSITIONS
            game_steps = []
            moves_history = []
            
            # è®¾ç½®å½“å‰ç©å®¶
            if black_first:
                current = BLACK
                first_player = "black"
            else:
                current = WHITE
                first_player = "white"
            
            # ç¬¬ä¸€æ­¥ä¸‹ä¸­å¿ƒï¼ˆå…ˆæ‰‹æ–¹ä¸‹ï¼‰
            center = BOARD_SIZE // 2
            first_move = center * BOARD_SIZE + center
            board[first_move] = current
            game_steps.append((board.copy(), current, first_move))
            moves_history.append((board.copy(), current, first_move))
            
            # åˆ‡æ¢åˆ°å¦ä¸€æ–¹
            current = 3 - current
            move_count = 1
            
            while move_count < max_moves:
                with torch.no_grad():
                    board_tensor = torch.tensor(board, dtype=torch.long, device=self.device).unsqueeze(0)
                    player_tensor = torch.tensor([0 if current == BLACK else 1], device=self.device)
                    policy_logits, _ = self.model.forward_with_mask(board_tensor, player_tensor)
                    
                    # ä¸­ç­‰æ¸©åº¦ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
                    temperature = 0.5
                    probs = F.softmax(policy_logits[0] / temperature, dim=-1).cpu().numpy()
                    
                    legals = get_legal_moves(board)
                    if not legals:
                        break
                    
                    # åªè€ƒè™‘åˆæ³•ä½ç½®
                    valid_probs = probs[legals]
                    if valid_probs.sum() > 0:
                        valid_probs = valid_probs / valid_probs.sum()
                        move = np.random.choice(legals, p=valid_probs)
                    else:
                        move = random.choice(legals)
                
                board[move] = current
                game_steps.append((board.copy(), current, move))
                moves_history.append((board.copy(), current, move))
                move_count += 1
                
                if check_win(board, current):
                    break
                
                current = 3 - current
            
            # ç¡®å®šèƒœè´Ÿ
            if check_win(board, BLACK):
                winner = 1
                winner_str = "black"
                if black_first:
                    black_win_count += 1
                else:
                    white_win_count += 1
            elif check_win(board, WHITE):
                winner = -1
                winner_str = "white"
                if not black_first:
                    black_win_count += 1
                else:
                    white_win_count += 1
            else:
                winner = 0
                winner_str = "draw"
                draw_count += 1
            
            # ä¿å­˜ç¬¬ä¸€å±€ç”¨äºæ‰“å°
            if game_idx == 0:
                first_game_board = moves_history
                first_game_moves = move_count
                first_game_winner = winner_str
            
            # ä¸ºæ¯ä¸€æ­¥è®¡ç®—å¥–åŠ±ï¼ˆåªæœ‰èƒœè´Ÿå±€æ‰å­¦ä¹ ï¼‰
            if winner != 0:
                gamma = 0.95
                T = len(game_steps)
                
                for t, (_, player, move) in enumerate(game_steps):
                    steps_to_end = T - t
                    reward = winner if player == BLACK else -winner
                    reward = reward * (gamma ** steps_to_end)
                    
                    games_data.append({
                        'board': game_steps[t][0],
                        'player': 0 if player == BLACK else 1,
                        'move': move,
                        'reward': reward,
                        'winner': winner
                    })
            
            # ä¿å­˜æ£‹å±€è®°å½•
            if save_games:
                games_records.append({
                    'timestamp': datetime.now().isoformat(),
                    'epoch': self.start_epoch,
                    'game_id': game_idx,
                    'first_player': first_player,
                    'winner': winner_str,
                    'moves': [(pos_to_str(move), 'black' if player == BLACK else 'white') 
                             for _, player, move in moves_history]
                })
        
        if save_games and games_records:
            save_games_to_file(games_records)
        
        total_games = black_win_count + white_win_count + draw_count
        win_rate = (black_win_count + white_win_count) / total_games if total_games > 0 else 0
        
        print(f"   å¯¹å±€ç»“æœ: é»‘èƒœ={black_win_count}, ç™½èƒœ={white_win_count}, å¹³å±€={draw_count}, èƒœç‡={win_rate:.2%}")
        print(f"   æœ‰æ•ˆè®­ç»ƒæ ·æœ¬: {len(games_data)} ä¸ª")
        
        return games_data, first_game_board, first_game_moves, first_game_winner
    
    def train_step(self, batch):
        boards = []
        players = []
        actions = []
        rewards = []
        legal_moves_list = []
        
        for item in batch:
            board = item['board']
            player = item['player']
            action = item['move']
            reward = item['reward']
            
            boards.append(board)
            players.append(player)
            actions.append(action)
            rewards.append(reward)
            
            nearby = get_nearby_moves(board, distance=2)
            if not nearby:
                center = BOARD_SIZE // 2
                nearby = [center * BOARD_SIZE + center]
            legal_moves_list.append(nearby)
        
        boards = torch.tensor(boards, dtype=torch.long, device=self.device)
        players = torch.tensor(players, dtype=torch.long, device=self.device)
        actions = torch.tensor(actions, dtype=torch.long, device=self.device)
        rewards = torch.tensor(rewards, dtype=torch.float, device=self.device)
        
        policy_logits, values = self.model.forward_with_mask(
            boards, players, legal_moves=legal_moves_list
        )
        
        policy_logits = torch.clamp(policy_logits, -20, 20)
        
        log_probs = F.log_softmax(policy_logits, dim=-1)
        action_log_probs = log_probs.gather(1, actions.unsqueeze(1)).squeeze()
        
        advantages = rewards - values.squeeze()
        advantages = torch.clamp(advantages, -0.2, 0.2)
        
        policy_loss = -(action_log_probs * advantages.detach()).mean()
        value_loss = F.mse_loss(values.squeeze(-1), rewards)
        
        entropy = 0
        for b in range(len(batch)):
            valid_indices = legal_moves_list[b]
            if valid_indices:
                valid_logits = policy_logits[b][valid_indices]
                valid_probs = F.softmax(valid_logits, dim=-1)
                valid_log_probs = F.log_softmax(valid_logits, dim=-1)
                entropy += -(valid_probs * valid_log_probs).sum()
        entropy = entropy / len(batch)
        entropy_bonus = 0.0002 * entropy
        
        total_loss = policy_loss + 0.5 * value_loss - entropy_bonus
        total_loss = torch.clamp(total_loss, min=0.01)
        
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)
        self.optimizer.step()
        
        return {
            'total_loss': total_loss.item(),
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'entropy': entropy.item(),
            'avg_reward': rewards.mean().item()
        }
    
    def train_epoch(self, games_per_epoch=20, batch_size=32):
        """æ¯ä¸ªepochï¼šè‡ªæˆ‘å¯¹å¼ˆç”Ÿæˆæ•°æ®å¹¶è®­ç»ƒ"""
        self.model.train()
        
        games_data, first_board, first_moves, first_winner = self.generate_self_play_games(
            num_games=games_per_epoch,
            save_games=True
        )
        
        # æ‰“å°ç¬¬ä¸€å±€æ£‹è°±
        if first_board:
            title = f"è‡ªæˆ‘å¯¹å¼ˆæ ·ä¾‹ - ç»“æœ: {first_winner} (å…±{first_moves}æ­¥)"
            print_game_board(first_board, title)
            print()
        
        if len(games_data) == 0:
            print("   è­¦å‘Šï¼šæœ¬è½®æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆè®­ç»ƒæ ·æœ¬")
            return {'total_loss': 0.0}
        
        random.shuffle(games_data)
        
        epoch_stats = defaultdict(float)
        batch_count = 0
        
        for start_idx in range(0, len(games_data), batch_size):
            batch = games_data[start_idx:start_idx + batch_size]
            stats = self.train_step(batch)
            
            for k, v in stats.items():
                epoch_stats[k] += v
            batch_count += 1
        
        for k in epoch_stats:
            epoch_stats[k] /= batch_count
        
        return epoch_stats

def main():
    parser = argparse.ArgumentParser(description='äº”å­æ£‹ Stage2 - è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒï¼ˆéšæœºå…ˆæ‰‹ï¼‰')
    parser.add_argument('--new', action='store_true', 
                       help='ä»å¤´å¼€å§‹è®­ç»ƒï¼ˆé»˜è®¤æ˜¯ç»§ç»­ä¸Šæ¬¡çš„è®­ç»ƒï¼‰')
    parser.add_argument('--checkpoint', '-ckpt', type=str, default=None,
                       help='æŒ‡å®šæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--epochs', '-e', type=int, default=50,
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤: 50)')
    parser.add_argument('--games', '-g', type=int, default=20,
                       help='æ¯è½®å¯¹å±€æ•° (é»˜è®¤: 20)')
    args = parser.parse_args()
    
    print("=" * 70)
    print("äº”å­æ£‹è®­ç»ƒ Stage2 - è‡ªæˆ‘å¯¹å¼ˆï¼ˆéšæœºå…ˆæ‰‹ï¼‰")
    print("=" * 70)
    
    device = get_best_device()
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    print("\n[1/3] åˆå§‹åŒ–æ¨¡å‹...")
    model = FearGreedWuziqiModel(
        d_model=128,
        nhead=4,
        num_layers=2,
        dim_feedforward=256
    )
    
    trainer = Stage2SelfPlayTrainer(model, device, lr=1e-5)
    
    if not args.new:
        checkpoint_file = None
        if args.checkpoint:
            checkpoint_file = args.checkpoint
        else:
            checkpoint_file = find_latest_checkpoint()
        
        if checkpoint_file and os.path.exists(checkpoint_file):
            trainer.load_checkpoint(checkpoint_file)
            print("âœ… ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ")
        else:
            # æ²¡æœ‰stage2æ£€æŸ¥ç‚¹ï¼Œå°è¯•åŠ è½½stage1æ¨¡å‹
            stage1_model = find_best_stage1_model()
            if stage1_model:
                try:
                    checkpoint = torch.load(stage1_model, map_location='cpu')
                    # åˆ¤æ–­æ˜¯checkpointè¿˜æ˜¯çº¯æƒé‡
                    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['model_state_dict'])
                    else:
                        model.load_state_dict(checkpoint)
                    model = model.to(device)
                    print(f"âœ… æˆåŠŸåŠ è½½ Stage1 æ¨¡å‹: {stage1_model}")
                except Exception as e:
                    print(f"âš ï¸ Stage1æ¨¡å‹åŠ è½½å¤±è´¥: {e}, ä½¿ç”¨éšæœºåˆå§‹åŒ–")
                    model = model.to(device)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°Stage1æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                model = model.to(device)
    else:
        print("ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒ")
        # å³ä½¿--newä¹Ÿå°è¯•åŠ è½½stage1
        stage1_model = find_best_stage1_model()
        if stage1_model:
            try:
                checkpoint = torch.load(stage1_model, map_location='cpu')
                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                model = model.to(device)
                print(f"âœ… åŠ è½½ Stage1 æ¨¡å‹ä½œä¸ºåˆå§‹æƒé‡: {stage1_model}")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å¤±è´¥: {e}, ä½¿ç”¨éšæœºåˆå§‹åŒ–")
                model = model.to(device)
        else:
            print("âš ï¸ æœªæ‰¾åˆ°Stage1æ¨¡å‹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
            model = model.to(device)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    print("\n[2/3] å¼€å§‹è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒ...")
    print("-" * 90)
    
    for epoch in range(trainer.start_epoch, args.epochs + 1):
        print(f"\nEpoch {epoch}/{args.epochs} - è‡ªæˆ‘å¯¹å¼ˆ...")
        train_stats = trainer.train_epoch(games_per_epoch=args.games, batch_size=32)
        
        print(f"\nEpoch {epoch:3d} | Loss: {train_stats.get('total_loss', 0):.4f}")
        
        trainer.save_checkpoint(epoch, f"wuziqi_stage2_checkpoint_epoch{epoch}.pth")
        trainer.scheduler.step()
    
    torch.save(model.cpu().state_dict(), "stage2/wuziqi_stage2_final.pth")
    
    print("\n[3/3] è®­ç»ƒå®Œæˆï¼")
    print("=" * 70)
    print("Stage2è®­ç»ƒå®Œæˆ")
    print("=" * 70)

if __name__ == "__main__":
    main()
