# model.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np

BOARD_SIZE = 15
BOARD_POSITIONS = BOARD_SIZE * BOARD_SIZE

class FearGreedWuziqiModel(nn.Module):
    """äº”å­æ£‹ææƒ§ä¸è´ªå©ªæ¨¡å‹ - ä¿®æ­£ç‰ˆ"""
    def __init__(self, d_model=128, nhead=4, num_layers=3, dim_feedforward=512, dropout=0.1):
        super().__init__()
        
        self.d_model = d_model
        self.board_size = BOARD_SIZE
        self.num_positions = BOARD_POSITIONS
        
        # åŸºç¡€åµŒå…¥
        self.state_embedding = nn.Embedding(3, d_model)
        self.pos_embedding = nn.Parameter(torch.randn(1, BOARD_POSITIONS, d_model) * 0.02)
        self.turn_embedding = nn.Embedding(2, d_model)
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True,
            activation='gelu'
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # LayerNorm
        self.norm = nn.LayerNorm(d_model)
        
        # ææƒ§å¤´ - æ¯ä¸ªä½ç½®è¾“å‡ºä¸€ä¸ªåˆ†æ•° (ä¿®æ­£)
        self.fear_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 1),  # ç›´æ¥è¾“å‡º1ä¸ªåˆ†æ•°
            nn.Sigmoid()  # ç¡®ä¿åœ¨0-1ä¹‹é—´
        )
        
        # è´ªå©ªå¤´ - æ¯ä¸ªä½ç½®è¾“å‡ºä¸€ä¸ªåˆ†æ•° (ä¿®æ­£)
        self.greed_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 1),
            nn.Sigmoid()
        )
        
        # æ™®é€šç­–ç•¥å¤´ - æ¯ä¸ªä½ç½®è¾“å‡ºä¸€ä¸ªåˆ†æ•° (ä¿®æ­£)
        self.normal_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 1),
            nn.Sigmoid()
        )
        
        # ä½ç½®èåˆå±‚ - ä¸ºæ¯ä¸ªä½ç½®ç‹¬ç«‹èåˆä¸‰ä¸ªåˆ†æ•°
        self.position_fusion = nn.Sequential(
            nn.Linear(3, d_model // 4),
            nn.ReLU(),
            nn.Linear(d_model // 4, 1)
        )
        
        # å…¨å±€ä»·å€¼å¤´
        self.value_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 1),
            nn.Tanh()
        )
        
        # æ³¨æ„åŠ›å¤´ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        self.attention_head = nn.Sequential(
            nn.Linear(d_model, 1),
            nn.Sigmoid()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, board, turn=None):
        """æ ‡å‡†å‰å‘ä¼ æ’­"""
        return self.forward_with_mask(board, turn)
    
    def forward_with_mask(self, board, turn=None, legal_moves=None, return_details=False):
        """
        å¸¦åˆæ³•ç§»åŠ¨æ©ç çš„å‰å‘ä¼ æ’­
        legal_moves: list of lists, æ¯ä¸ªæ ·æœ¬çš„åˆæ³•ä½ç½®åˆ—è¡¨
        """
        batch_size = board.size(0)
        
        # åŸºç¡€åµŒå…¥
        emb = self.state_embedding(board) * math.sqrt(self.d_model)
        emb = emb + self.pos_embedding
        
        # åŠ å…¥å›åˆä¿¡æ¯ (æƒé‡åŠ å¤§)
        if turn is not None:
            turn_emb = self.turn_embedding(turn).unsqueeze(1).expand(-1, self.num_positions, -1)
            emb = emb + turn_emb  # å»æ‰0.1çš„æƒé‡ï¼Œç›´æ¥åŠ 
        
        # Transformerç¼–ç å™¨
        memory = self.encoder(emb)
        memory = self.norm(memory)
        
        # å…¨å±€ç‰¹å¾ (ç”¨äºä»·å€¼è¯„ä¼°)
        global_feat = memory.mean(dim=1)
        
        # ä¸‰ä¸ªå¤´ - æ¯ä¸ªä½ç½®è¾“å‡ºä¸€ä¸ªåˆ†æ•° [batch, 225, 1]
        fear_scores = self.fear_head(memory).squeeze(-1)      # [batch, 225]
        greed_scores = self.greed_head(memory).squeeze(-1)    # [batch, 225]
        normal_scores = self.normal_head(memory).squeeze(-1)  # [batch, 225]
        
        # ä¸ºæ¯ä¸ªä½ç½®ç‹¬ç«‹èåˆä¸‰ä¸ªåˆ†æ•°
        # å°†ä¸‰ä¸ªåˆ†æ•°å †å æˆ [batch, 225, 3]
        combined_scores = torch.stack([fear_scores, greed_scores, normal_scores], dim=-1)
        
        # å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åº”ç”¨èåˆå±‚
        policy_logits = self.position_fusion(combined_scores).squeeze(-1)  # [batch, 225]
        
        # å¦‚æœæœ‰legal_movesï¼Œåªè®¡ç®—åˆæ³•ä½ç½®çš„æ¦‚ç‡
        if legal_moves is not None:
            mask = torch.ones_like(policy_logits) * float('-inf')
            for b in range(batch_size):
                for pos in legal_moves[b]:
                    if 0 <= pos < self.num_positions:
                        mask[b, pos] = 0
            policy_logits = policy_logits + mask
        
        # ä»·å€¼è¯„ä¼°
        value = self.value_head(global_feat)
        
        if return_details:
            return {
                'policy': policy_logits,
                'value': value,
                'fear_scores': fear_scores,
                'greed_scores': greed_scores,
                'normal_scores': normal_scores,
                'attention': self.attention_head(global_feat)
            }
        
        return policy_logits, value
    
    def decide_move_fast(self, board, player, device='cpu', debug=False):
        """å¿«é€Ÿå†³ç­– - åªè€ƒè™‘é™„è¿‘ä½ç½®"""
        self.eval()
        with torch.no_grad():
            from game import get_nearby_moves
            
            nearby = get_nearby_moves(board, distance=2)
            if not nearby:
                center = BOARD_SIZE // 2
                center_pos = center * BOARD_SIZE + center
                return {
                    'move': center_pos,
                    'policy': np.zeros(BOARD_POSITIONS),
                    'fear': np.zeros(BOARD_POSITIONS),
                    'greed': np.zeros(BOARD_POSITIONS),
                    'value': 0.0,
                    'attention': 0.5,
                    'nearby': [center_pos]
                }
            
            board_tensor = torch.tensor(board, dtype=torch.long, device=device).unsqueeze(0)
            turn = torch.tensor([0 if player == 1 else 1], device=device)
            
            details = self.forward_with_mask(
                board_tensor, turn, legal_moves=[nearby], return_details=True
            )
            
            policy = F.softmax(details['policy'][0], dim=-1).cpu().numpy()
            fear_scores = details['fear_scores'][0].cpu().numpy()
            greed_scores = details['greed_scores'][0].cpu().numpy()
            
            # ä»é™„è¿‘ä½ç½®ä¸­é€‰æ‹©æœ€ä½³ä½ç½®
            nearby_probs = {pos: policy[pos] for pos in nearby}
            best_move = max(nearby_probs.items(), key=lambda x: x[1])[0]
            
            value = details['value'][0].item()
            attention = details['attention'][0].item()
            
            if debug:
                print(f"\nğŸ¯ å€™é€‰ä½ç½® ({len(nearby)}ä¸ª):")
                sorted_moves = sorted(nearby_probs.items(), key=lambda x: x[1], reverse=True)[:5]
                for pos, prob in sorted_moves:
                    from game import pos_to_str
                    print(f"   {pos_to_str(pos)}: æ¦‚ç‡={prob:.3f}, ææƒ§={fear_scores[pos]:.3f}, è´ªå©ª={greed_scores[pos]:.3f}")
            
            return {
                'move': best_move,
                'policy': policy,
                'fear': fear_scores,
                'greed': greed_scores,
                'value': value,
                'attention': attention,
                'nearby': nearby
            }
