# model.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np

BOARD_SIZE = 15
BOARD_POSITIONS = BOARD_SIZE * BOARD_SIZE

class AttentionFusion(nn.Module):
    """æ³¨æ„åŠ›èåˆæ¨¡å— - ä¸ºæ¯ä¸ªå¤´åŠ¨æ€åˆ†é…æƒé‡"""
    def __init__(self, feat_dim=32, d_model=64, dropout=0.1):
        super().__init__()
        self.feat_dim = feat_dim
        self.total_dim = feat_dim * 3  # 96
        
        # ä¸ºæ¯ä¸ªå¤´ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        self.fear_attn = nn.Linear(feat_dim, 1)
        self.greed_attn = nn.Linear(feat_dim, 1)
        self.normal_attn = nn.Linear(feat_dim, 1)
        
        # èåˆåçš„å¤„ç†
        self.fusion = nn.Sequential(
            nn.Linear(self.total_dim, d_model),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, 1)
        )
    
    def forward(self, fear, greed, normal):
        # fear, greed, normal: [batch, 225, 32]
        
        # æ¯ä¸ªå¤´è‡ªå·±å†³å®šè‡ªå·±çš„é‡è¦æ€§
        fear_weight = torch.sigmoid(self.fear_attn(fear))  # [batch, 225, 1]
        greed_weight = torch.sigmoid(self.greed_attn(greed))
        normal_weight = torch.sigmoid(self.normal_attn(normal))
        
        # åŠ æƒç‰¹å¾
        weighted_fear = fear * fear_weight
        weighted_greed = greed * greed_weight
        weighted_normal = normal * normal_weight
        
        # æ‹¼æ¥
        combined = torch.cat([weighted_fear, weighted_greed, weighted_normal], dim=-1)  # [batch, 225, 96]
        
        # èåˆ
        return self.fusion(combined)  # [batch, 225, 1]

class FearGreedWuziqiModel(nn.Module):
    """äº”å­æ£‹ææƒ§ä¸è´ªå©ªæ¨¡å‹ - æ³¨æ„åŠ›èåˆç‰ˆ"""
    def __init__(self, d_model=128, nhead=4, num_layers=2, dim_feedforward=256, dropout=0.1):
        super().__init__()
        
        self.d_model = d_model
        self.board_size = BOARD_SIZE
        self.num_positions = BOARD_POSITIONS
        
        # åŸºç¡€åµŒå…¥
        self.state_embedding = nn.Embedding(3, d_model)
        self.pos_embedding = nn.Parameter(torch.randn(1, BOARD_POSITIONS, d_model) * 0.02)
        self.turn_embedding = nn.Embedding(2, d_model)
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True,
            activation='gelu'
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # LayerNorm
        self.norm = nn.LayerNorm(d_model)
        
        # ææƒ§å¤´ - æ¯ä¸ªä½ç½®è¾“å‡º32ç»´ç‰¹å¾
        self.fear_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, d_model // 4),  # 32ç»´
            nn.ReLU(),
        )
        
        # è´ªå©ªå¤´ - æ¯ä¸ªä½ç½®è¾“å‡º32ç»´ç‰¹å¾
        self.greed_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, d_model // 4),  # 32ç»´
            nn.ReLU(),
        )
        
        # æ™®é€šç­–ç•¥å¤´ - æ¯ä¸ªä½ç½®è¾“å‡º32ç»´ç‰¹å¾
        self.normal_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, d_model // 4),  # 32ç»´
            nn.ReLU(),
        )
        
        # æ³¨æ„åŠ›èåˆå±‚
        self.position_fusion = AttentionFusion(feat_dim=32, d_model=64, dropout=dropout)
        
        # ä»·å€¼å¤´ï¼ˆç”¨å…¨å±€ç‰¹å¾ï¼‰
        self.value_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 1),
            nn.Tanh()
        )
        
        # æ³¨æ„åŠ›å¤´ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        self.attention_head = nn.Sequential(
            nn.Linear(d_model, 1),
            nn.Sigmoid()
        )
        
        self._init_weights()
    
    def _init_weights(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, board, turn=None):
        """æ ‡å‡†å‰å‘ä¼ æ’­"""
        return self.forward_with_mask(board, turn)
    
    def forward_with_mask(self, board, turn=None, legal_moves=None, return_details=False):
        """
        å¸¦åˆæ³•ç§»åŠ¨æ©ç çš„å‰å‘ä¼ æ’­ - æ³¨æ„åŠ›èåˆç‰ˆ
        legal_moves: list of lists, æ¯ä¸ªæ ·æœ¬çš„åˆæ³•ä½ç½®åˆ—è¡¨
        """
        batch_size = board.size(0)
        
        # åŸºç¡€åµŒå…¥
        emb = self.state_embedding(board) * math.sqrt(self.d_model)
        emb = emb + self.pos_embedding
        
        # åŠ å…¥å›åˆä¿¡æ¯
        if turn is not None:
            turn_emb = self.turn_embedding(turn).unsqueeze(1).expand(-1, self.num_positions, -1)
            emb = emb + turn_emb
        
        # Transformerç¼–ç å™¨
        memory = self.encoder(emb)
        memory = self.norm(memory)
        
        # å…¨å±€ç‰¹å¾ï¼ˆç”¨äºä»·å€¼è¯„ä¼°ï¼‰
        global_feat = memory.mean(dim=1)
        
        # ä¸‰ä¸ªå¤´ - æ¯ä¸ªä½ç½®è¾“å‡º32ç»´ç‰¹å¾ [batch, 225, 32]
        fear_features = self.fear_head(memory)
        greed_features = self.greed_head(memory)
        normal_features = self.normal_head(memory)
        
        # æ³¨æ„åŠ›èåˆ - å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹å†³ç­–
        position_scores = self.position_fusion(fear_features, greed_features, normal_features)  # [batch, 225, 1]
        
        # å»æ‰æœ€åä¸€ç»´ï¼Œå¾—åˆ° [batch, 225]
        policy_logits = position_scores.squeeze(-1)
        
        # å¦‚æœæœ‰legal_movesï¼Œåªè®¡ç®—åˆæ³•ä½ç½®çš„æ¦‚ç‡
        if legal_moves is not None:
            mask = torch.ones_like(policy_logits) * float('-inf')
            for b in range(batch_size):
                for pos in legal_moves[b]:
                    if 0 <= pos < self.num_positions:
                        mask[b, pos] = 0
            policy_logits = policy_logits + mask
        
        # ä»·å€¼è¯„ä¼°
        value = self.value_head(global_feat)
        
        if return_details:
            # ä¸ºäº†å¯è§†åŒ–ï¼Œå¯¹ç‰¹å¾å–å¹³å‡å¾—åˆ°æ¯ä¸ªä½ç½®çš„åˆ†æ•°ï¼ˆå¹¶å‹ç¼©åˆ°0-1ï¼‰
            # fear_scores = torch.sigmoid(fear_features.mean(dim=-1))      # [batch, 225]
            # greed_scores = torch.sigmoid(greed_features.mean(dim=-1))    # [batch, 225]
            # normal_scores = torch.sigmoid(normal_features.mean(dim=-1))  # [batch, 225]
            fear_scores = fear_features.mean(dim=-1)      # [batch, 225]
            greed_scores = greed_features.mean(dim=-1)    # [batch, 225]
            normal_scores = normal_features.mean(dim=-1)  # [batch, 225]
            
            return {
                'policy': policy_logits,
                'value': value,
                'fear_scores': fear_scores,
                'greed_scores': greed_scores,
                'normal_scores': normal_scores,
                'attention': self.attention_head(global_feat)
            }
        
        return policy_logits, value
    
    def decide_move_fast(self, board, player, device='cpu', debug=False):
        """å¿«é€Ÿå†³ç­– - åªè€ƒè™‘é™„è¿‘ä½ç½®"""
        self.eval()
        with torch.no_grad():
            from game import get_nearby_moves
            
            nearby = get_nearby_moves(board, distance=2)
            if not nearby:
                center = BOARD_SIZE // 2
                center_pos = center * BOARD_SIZE + center
                return {
                    'move': center_pos,
                    'policy': np.zeros(BOARD_POSITIONS),
                    'fear': np.zeros(BOARD_POSITIONS),
                    'greed': np.zeros(BOARD_POSITIONS),
                    'normal': np.zeros(BOARD_POSITIONS),
                    'value': 0.0,
                    'attention': 0.5,
                    'nearby': [center_pos]
                }
            
            board_tensor = torch.tensor(board, dtype=torch.long, device=device).unsqueeze(0)
            turn = torch.tensor([0 if player == 1 else 1], device=device)
            
            details = self.forward_with_mask(
                board_tensor, turn, legal_moves=[nearby], return_details=True
            )
            
            policy = F.softmax(details['policy'][0], dim=-1).cpu().numpy()
            fear_scores = details['fear_scores'][0].cpu().numpy()
            greed_scores = details['greed_scores'][0].cpu().numpy()
            normal_scores = details['normal_scores'][0].cpu().numpy()
            
            nearby_probs = {pos: policy[pos] for pos in nearby}
            best_move = max(nearby_probs.items(), key=lambda x: x[1])[0]
            
            value = details['value'][0].item()
            attention = details['attention'][0].item()
            
            if debug:
                print(f"\nğŸ¯ å€™é€‰ä½ç½® ({len(nearby)}ä¸ª):")
                sorted_moves = sorted(nearby_probs.items(), key=lambda x: x[1], reverse=True)[:5]
                for pos, prob in sorted_moves:
                    from game import pos_to_str
                    print(f"   {pos_to_str(pos)}: æ¦‚ç‡={prob:.3f}, ææƒ§={fear_scores[pos]:.3f}, è´ªå©ª={greed_scores[pos]:.3f}, æ™®é€š={normal_scores[pos]:.3f}")
            
            return {
                'move': best_move,
                'policy': policy,
                'fear': fear_scores,
                'greed': greed_scores,
                'normal': normal_scores,
                'value': value,
                'attention': attention,
                'nearby': nearby
            }
